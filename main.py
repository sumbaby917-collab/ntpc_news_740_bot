import feedparser
import requests
import datetime
import os
import urllib.parse
import time
import json
import re
from html import escape, unescape

import google.generativeai as genai

# =========================
# 0) åŸºæœ¬è¨­å®š
# =========================
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
GEMINI_KEY = os.getenv('GEMINI_API_KEY')

assert TELEGRAM_TOKEN, "ç¼ºå°‘ TELEGRAM_TOKEN"
assert CHAT_ID, "ç¼ºå°‘ TELEGRAM_CHAT_ID"

# Gemini
if GEMINI_KEY:
    genai.configure(api_key=GEMINI_KEY)

MODEL_CANDIDATES = [
    "models/gemini-2.5-flash",
    "models/gemini-2.5-pro",
]

# =========================
# 1) æŸ¥è©¢ï¼šæ–°åŒ—å„ªå…ˆ + å…¨åœ‹æ“´æ•£
# =========================
QUERY_POOLS = {
    "äº¤é€šæ”¿å‹™": {
        "ntpc": "æ–°åŒ— (äº¤é€šå®‰å…¨ OR è¡Œäºº OR é€šå­¸å·· OR äº‹æ•… OR é…’é§• OR æ·¡æ±Ÿå¤§æ©‹)",
        "national": "(äº¤é€šå®‰å…¨ OR è¡Œäººå®‰å…¨ OR é€šå­¸å·· OR äº‹æ•… OR é…’é§• OR è·¯å£æ”¹å–„)"
    },
    "æ•™è‚²æ¥­å‹™": {
        "ntpc": "æ–°åŒ— (è£œç¿’ç­ OR æœªç«‹æ¡ˆè£œç¿’ç­ OR èª²å¾Œç…§é¡§ OR çµ‚èº«å­¸ç¿’ OR æŠ€è·)",
        "national": "(è£œç¿’ç­ OR æœªç«‹æ¡ˆè£œç¿’ç­ OR èª²å¾Œç…§é¡§ OR çµ‚èº«å­¸ç¿’ OR æŠ€è·)"
    },
}

# =========================
# 2) æ›´æ–°ä¿¡è™Ÿï¼ˆèˆŠèä½†æœ‰æ–°é€²åº¦æ‰å…è¨±ï¼‰
# =========================
UPDATE_HINTS = [
    "æœ€æ–°", "æ›´æ–°", "çºŒ", "å†", "äºŒåº¦", "ç¬¬ä¸‰æ¬¡", "è¿½åŠ ", "åŠ é‡", "æ“´å¤§",
    "èµ·è¨´", "åˆ¤æ±º", "è£å®š", "åˆ¤åˆ‘", "ç§»é€", "å‹’ä»¤", "åœæ¥­", "æ’¤ç…§",
    "å†ç½°", "çºŒç½°", "ç´¯ç½°", "é‡ç½°", "ç¨½æŸ¥", "æŸ¥ç²", "é–‹ç½°", "ä¸æ€•ç½°"
]

NTPC_HINTS = [
    "æ–°åŒ—", "æ–°åŒ—å¸‚", "æ¿æ©‹", "æ–°èŠ", "ä¸­å’Œ", "æ°¸å’Œ", "ä¸‰é‡", "è˜†æ´²",
    "æ–°åº—", "åœŸåŸ", "æ¨¹æ—", "é¶¯æ­Œ", "ä¸‰å³½", "æ—å£", "æ·¡æ°´", "æ±æ­¢", "ç‘èŠ³",
    "ä¾¯å‹å®œ"
]

# =========================
# 3) Cacheï¼šè·¨æ¬¡åŸ·è¡Œé¿å…é‡è¤‡æ¨æ’­
# =========================
CACHE_FILE = "sent_cache.json"
CACHE_TTL_DAYS = 7  # ä¿ç•™ 7 å¤©ï¼Œé¿å…ä¸€é€±å…§é‡è¤‡æ¨æ’­åŒä¸€å‰‡

def load_cache():
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        # data: { "url": {"ts": epoch}, ... }
        if not isinstance(data, dict):
            return {}
        return data
    except Exception:
        return {}

def save_cache(cache: dict):
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception:
        # å¯«å…¥å¤±æ•—ä¹Ÿä¸å½±éŸ¿ä¸»æµç¨‹
        pass

def prune_cache(cache: dict):
    now = int(time.time())
    ttl = CACHE_TTL_DAYS * 86400
    keys = list(cache.keys())
    for k in keys:
        ts = cache.get(k, {}).get("ts", 0)
        if now - ts > ttl:
            cache.pop(k, None)

def cache_seen(cache: dict, canonical_url: str) -> bool:
    if not canonical_url:
        return False
    return canonical_url in cache

def cache_mark(cache: dict, canonical_url: str):
    if not canonical_url:
        return
    cache[canonical_url] = {"ts": int(time.time())}

# =========================
# 4) åŸºç¤å·¥å…·ï¼šå®‰å…¨ requestï¼ˆé¿å… workflow è®Šç´…ï¼‰
# =========================
def safe_get(url: str, timeout=12):
    try:
        return requests.get(
            url,
            timeout=timeout,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0"}
        )
    except Exception as e:
        print("WARN safe_get failed:", type(e).__name__, str(e)[:120])
        return None

# =========================
# 5) æ™‚é–“åˆ¤æ–·ï¼šè¿‘ 24h æˆ–ï¼ˆèˆŠèä½†æ¨™é¡Œé¡¯ç¤ºæœ‰æ›´æ–°ï¼‰
# =========================
def get_entry_time_utc(entry):
    t = None
    try:
        if getattr(entry, "published_parsed", None):
            t = datetime.datetime.fromtimestamp(
                time.mktime(entry.published_parsed),
                tz=datetime.timezone.utc
            )
        elif getattr(entry, "updated_parsed", None):
            t = datetime.datetime.fromtimestamp(
                time.mktime(entry.updated_parsed),
                tz=datetime.timezone.utc
            )
    except Exception:
        return None
    return t

def is_update_story(title: str) -> bool:
    return any(k in (title or "") for k in UPDATE_HINTS)

def is_recent_or_update(entry, hours=24) -> bool:
    t = get_entry_time_utc(entry)
    if t is None:
        return True
    now = datetime.datetime.now(datetime.timezone.utc)
    age = now - t
    if age <= datetime.timedelta(hours=hours):
        return True
    return is_update_story(getattr(entry, "title", ""))

# =========================
# 6) Google News é€£çµè§£åŒ…ï¼šç¢ºå¯¦å–åˆ°å¤–ç«™åŸæ–‡
# =========================
def extract_external_url_from_google_news_html(html: str):
    if not html:
        return None
    html = unescape(html)

    # å„ªå…ˆæŠ“ href="https://xxx" ä¸”é google ç¶²åŸŸ
    candidates = re.findall(r'href="(https?://[^"]+)"', html)
    for u in candidates:
        if any(bad in u for bad in [
            "news.google.com", "accounts.google.com", "policies.google.com",
            "support.google.com", "google.com"
        ]):
            continue
        return u

    # å‚™æ´ï¼šæŠ“ url= åƒæ•¸
    m = re.search(r"[?&]url=(https?%3A%2F%2F[^&]+)", html)
    if m:
        return urllib.parse.unquote(m.group(1))
    return None

def resolve_to_canonical_news_url(url: str) -> str:
    if not url:
        return url

    r = safe_get(url)
    if not r:
        return url

    final_url = r.url

    # è‹¥å·²ç¶“æ˜¯å¤–ç«™
    if "news.google.com" not in final_url:
        parsed = urllib.parse.urlparse(final_url)
        qs = urllib.parse.parse_qs(parsed.query)
        if "url" in qs and qs["url"]:
            return qs["url"][0]
        return final_url

    # é‚„åœåœ¨ Google Newsï¼šå¾ HTML æŠ“å¤–ç«™
    ext = extract_external_url_from_google_news_html(r.text)
    if ext:
        return ext

    return final_url

def get_best_link(entry) -> str:
    # 1) source.href è‹¥æ˜¯å¤–ç«™
    if getattr(entry, "source", None) and getattr(entry.source, "href", None):
        href = entry.source.href
        if href and "news.google.com" not in href:
            return href

    # 2) links å…§æ‰¾å¤–ç«™
    for l in getattr(entry, "links", []) or []:
        href = l.get("href")
        if href and "news.google.com" not in href:
            return href

    # 3) entry.link è§£åŒ…
    return resolve_to_canonical_news_url(getattr(entry, "link", ""))

# =========================
# 7) å»é‡ï¼šcanonical URL å„ªå…ˆï¼›æ¨™é¡Œè¦ç¯„åŒ–å‚™æ´
# =========================
def normalize_title(title: str) -> str:
    t = (title or "").strip()
    # å»æ‰å¸¸è¦‹å°¾ç¶´ä¾†æº
    t = re.split(
        r"\s*[-ï½œ|]\s*(?:è¯åˆæ–°èç¶²|udn|é¡é€±åˆŠ|ä¸­æ™‚|ä¸­åœ‹æ™‚å ±|è‡ªç”±æ™‚å ±|ETtoday|TVBS|ä¸‰ç«‹|Yahoo|NOWnews|CTWANT|é¢¨å‚³åª’|å·¥å•†æ™‚å ±|å¤ªå ±).*$",
        t,
        maxsplit=1
    )[0]
    t = re.sub(r"\s+", " ", t)
    return t

def dedupe_key(entry):
    title = getattr(entry, "title", "") or ""
    canonical = get_best_link(entry) or ""
    if canonical and "news.google.com" not in canonical:
        return ("url", canonical)
    return ("title", normalize_title(title))

# =========================
# 8) æ–°åŒ—å„ªå…ˆæ’åºï¼šæ–°åŒ—åœ¨å‰ï¼Œå…¶æ¬¡æ™‚é–“æ–°è¿‘åº¦
# =========================
def is_ntpc_related(title: str) -> bool:
    return any(k in (title or "") for k in NTPC_HINTS)

def sort_key(entry):
    title = getattr(entry, "title", "") or ""
    t = get_entry_time_utc(entry)
    ntpc_rank = 0 if is_ntpc_related(title) else 1
    if t is None:
        time_rank = 999999999
    else:
        now = datetime.datetime.now(datetime.timezone.utc)
        time_rank = int((now - t).total_seconds())
    return (ntpc_rank, time_rank)

# =========================
# 9) AI æ‘˜è¦ï¼ˆsoft-failï¼ŒAI å£äº†ä¹Ÿä¸å½±éŸ¿æ™¨å ±ï¼‰
# =========================
def get_ai_analysis(title: str) -> str:
    if not GEMINI_KEY:
        return "ï¼ˆAIï¼‰æœªè¨­å®š GEMINI_API_KEYï¼Œæš«ä»¥äººå·¥åˆ¤è®€ç‚ºä¸»ã€‚"

    prompt = (
        "è«‹ä»¥æ–°åŒ—å¸‚æ”¿åºœæ•™è‚²å±€æ”¿ç­–æ²»ç†è¦–è§’ï¼Œ"
        "é‡å°æ–°èæ¨™é¡Œç”¢å‡ºï¼š"
        "ï¼ˆä¸€ï¼‰å…©å¥é‡é»æ‘˜è¦ï¼›ï¼ˆäºŒï¼‰ä¸€é …è¡Œæ”¿å› æ‡‰å»ºè­°ã€‚"
        "èªæ°£æ­£å¼ã€å°ˆæ¥­ã€å¯ä¾›å±€å…§ç°¡å ±ã€‚\n"
        f"æ–°èæ¨™é¡Œï¼š{title}"
    )

    last_error = None
    for model_id in MODEL_CANDIDATES:
        try:
            model = genai.GenerativeModel(model_id)
            resp = model.generate_content(prompt)
            if resp and getattr(resp, "text", None):
                return resp.text.strip()
        except Exception as e:
            last_error = e
            continue

    # AI å¤±æ•—ä¸è®“æµç¨‹ä¸­æ–·
    return f"ï¼ˆAIï¼‰æš«æ™‚ç„¡æ³•ç”¢å‡ºï¼Œç³»çµ±å°‡æŒçºŒé‡è©¦ï¼ˆ{type(last_error).__name__}ï¼‰ã€‚"

# =========================
# 10) RSS æŠ“å–
# =========================
def fetch_entries(query: str):
    safe_query = urllib.parse.quote_plus(query)
    rss_url = f"https://news.google.com/rss/search?q={safe_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    try:
        feed = feedparser.parse(rss_url)
        return feed.entries or []
    except Exception as e:
        print("WARN feedparser failed:", type(e).__name__, str(e)[:120])
        return []

# =========================
# 11) ç”¢ç”Ÿå ±å‘Šï¼šæ–°åŒ—å„ªå…ˆã€å…¨åœ‹è£œè¶³ã€å»é‡ã€è·¨æ—¥ä¸é‡è¤‡ã€å–å‰3
# =========================
def generate_report():
    today = datetime.date.today().isoformat()
    report = f"ğŸ“‹ <b>æ•™è‚²è¼¿æƒ…å ±å‘Šï¼ˆæ–°åŒ—å„ªå…ˆï¼‹å…¨åœ‹å‹•æ…‹ï¼‰({today})</b>\n"
    report += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

    cache = load_cache()
    prune_cache(cache)

    for label, pools in QUERY_POOLS.items():
        report += f"\nğŸ” <b>é¡åˆ¥ï¼š{escape(label)}</b>\n"

        entries = []
        entries += fetch_entries(pools["ntpc"])
        entries += fetch_entries(pools["national"])

        if not entries:
            report += "ä»Šæ—¥æš«ç„¡ç›¸é—œæ–°èã€‚\n"
            continue

        # 1) æ–°/æ›´æ–°éæ¿¾
        entries = [e for e in entries if is_recent_or_update(e, hours=24)]
        if not entries:
            report += "è¿‘ 24 å°æ™‚ï¼ˆå«æ›´æ–°é€²åº¦ï¼‰æœªç¯©é¸åˆ°ç¬¦åˆæ¢ä»¶ä¹‹æ–°èã€‚\n"
            continue

        # 2) å»é‡ï¼ˆåŒ runï¼‰
        seen = set()
        uniq = []
        for e in entries:
            k = dedupe_key(e)
            if k in seen:
                continue
            seen.add(k)
            uniq.append(e)

        # 3) æ–°åŒ—å„ªå…ˆ + è¶Šæ–°è¶Šå‰
        uniq.sort(key=sort_key)

        # 4) è·¨æ—¥/è·¨ run ä¸é‡è¤‡ï¼ˆä»¥ canonical url ç‚ºæº–ï¼‰
        picked = 0
        for e in uniq:
            title = (getattr(e, "title", "") or "").strip()
            canonical = get_best_link(e)

            # canonical å–ä¸åˆ°æ™‚ä»å¯å‡ºï¼Œä½†ç„¡æ³•é€² cache å»é‡
            if canonical and cache_seen(cache, canonical):
                continue

            analysis = get_ai_analysis(title)
            link = canonical or getattr(e, "link", "")

            report += f"ğŸ“ <b>æ–°è</b>ï¼š{escape(title)}\n"
            report += f"ğŸ’¡ {escape(analysis)}\n"
            report += f"ğŸ”— <a href=\"{escape(link)}\">åŸæ–‡é€£çµ</a>\n"
            report += "--------------------\n"

            if canonical:
                cache_mark(cache, canonical)

            picked += 1
            if picked >= 3:
                break

        if picked == 0:
            report += "ä»Šæ—¥æš«ç„¡å¯æ¨æ’­æ–°èï¼ˆå·²æ’é™¤é‡è¤‡æˆ–èˆŠèä¸”ç„¡æ›´æ–°è€…ï¼‰ã€‚\n"

    save_cache(cache)
    return report

# =========================
# 12) ä¸»ç¨‹å¼ï¼šTelegram ç™¼é€ï¼ˆsoft-failï¼‰
# =========================
if __name__ == "__main__":
    final_report = generate_report()

    try:
        r = requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            data={
                "chat_id": CHAT_ID,
                "text": final_report,
                "parse_mode": "HTML",
                "disable_web_page_preview": True
            },
            timeout=20
        )
        if not r.ok:
            print("Telegram ç™¼é€å¤±æ•—ï¼š", r.status_code, r.text)
    except Exception as e:
        print("WARN Telegram request failed:", type(e).__name__, str(e)[:120])
